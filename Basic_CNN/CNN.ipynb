{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# This notebook contains all the necessary objects and functions to create and train a basic CNN model using the Keras API from Tensorflow. It also contains cells for datapreprocessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The next cell will import all necessary libraries and API and allow for GPU memory growth (not recommended if not familiar with how memory is allocated in Tensorflow)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'    #To not print info mesages\n",
    "import tensorflow as tf                     #import necessary Libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:    #To allow GPU memory growth\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "source": [
    "### This cell creates the model as an object, using the Keras layers. Note that this version includes my final code with advanced techniques such as batch normalization and max pooling. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Model--------------------------------------------------------------------------------------------------------------------\n",
    "class MyModel(tf.keras.Model):                          #Make the thing\n",
    "    def __init__(self, kernels, filters):               #Put smaller things in it\n",
    "        super(MyModel, self).__init__(name=\"basicCNN\")\n",
    "        kernel1, kernel2, kernel3, kernel4 = kernels             #Decide the shape of the smaller things\n",
    "        filter1, filter2, filter3, filter4 = filters             #Decide how many there are\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filter1, kernel1, padding='same')  #Now stack'em \n",
    "        self.bnorm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.act1 = tf.keras.layers.Activation(tf.nn.relu)                      #Non-lin otherwhise you just compute f(x)=W.x\n",
    "        self.mxpool1 = tf.keras.layers.MaxPool2D(pool_size = (2,2), padding = 'valid')  #Now shrink the thingy\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filter2, kernel2, padding='same')  #And again\n",
    "        self.bnorm2 = tf.keras.layers.BatchNormalization()\n",
    "        self.act2 = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.mxpool2 = tf.keras.layers.MaxPool2D(pool_size = (2,2), padding = 'valid')\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filter3, kernel3, padding='same')  #And again\n",
    "        self.bnorm3 = tf.keras.layers.BatchNormalization()\n",
    "        self.act3 = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.mxpool3 = tf.keras.layers.MaxPool2D(pool_size = (2,2), padding = 'valid')\n",
    "\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filter4, kernel4, padding='same')  #And again\n",
    "        self.bnorm4 = tf.keras.layers.BatchNormalization()\n",
    "        self.act4 = tf.keras.layers.Activation(tf.nn.relu)\n",
    "        self.mxpool4 = tf.keras.layers.MaxPool2D(pool_size = (2,2), padding = 'valid')\n",
    "\n",
    "        self.flat = tf.keras.layers.Flatten()           #Now decide what it is you are actually looking at\n",
    "        self.FC1 = tf.keras.layers.Dense(64)\n",
    "        self.FCo = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x = self.conv2a(input)\n",
    "        x = self.bnorm1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.mxpool1(x)\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.bnorm2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.mxpool2(x)\n",
    "\n",
    "        x = self.conv2c(x)\n",
    "        x = self.bnorm3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.mxpool3(x)\n",
    "\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bnorm4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.mxpool4(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = self.FC1(x)\n",
    "        x = self.FCo(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Variables----------------------------------------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "kernels = [(4,4), (4,4), (4,4), (4,4)]  #A list for the kernel sizes\n",
    "filters = [16, 32, 64, 128]             #A list for the filteer sizes\n",
    "base_learning_rate = 1e-3"
   ]
  },
  {
   "source": [
    "### In order to make the data understandable for the network and to ensure a smoother training, we preprocess the data by normalizing it and making batches with sizes defined above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#---Custom Functions --------------------------------------------------------------------------------------------------------\n",
    "def create_set(limits, data, b_s):      #Function to create a set of images with batches. Data is an array containing the data. B_s is the batch size.\n",
    "    i_min, i_max = limits               #limits is to be of shape (i_min, i_max)\n",
    "    new_set = []\n",
    "    for i in range(i_min, i_max, b_s):\n",
    "        new_set.append(data[i : i+b_s])\n",
    "    return new_set"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Data Preprocessing-------------------------------------------------------------------------------------------------------\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() #import dataset\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0       #Normalization pixels\n",
    "\n",
    "rng_state = np.random.get_state()\n",
    "np.random.shuffle(x_train)                              #Shuffle it before making train, val and test sets\n",
    "np.random.set_state(rng_state)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")    #Add an axis for the single channel\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")      #Make test set\n",
    "\n",
    "y_train = tf.keras.backend.one_hot(y_train, 10)\n",
    "y_test = tf.keras.backend.one_hot(y_test, 10)      \n",
    "#One-hot encodes a prediction vector containing integers from 0 to 9 (because there are 10 categories) into a vector with size 10 caintaining 1 at the place for the correct class and 0 elsewhere.\n",
    "\n",
    "#Substract mean image. This is not indispensable but allows for a better generalization potential for the network\n",
    "x_test -= np.mean(np.mean(x_train.reshape(60000,784), axis=1), axis=0)\n",
    "x_train -= np.mean(np.mean(x_train.reshape(60000,784), axis=1), axis=0)\n",
    "\n",
    "x_train_dataset = create_set((0, 60000), x_train, 32)   #Make the batches\n",
    "y_train_dataset = create_set((0, 60000), y_train, 32)\n",
    "train_dataset = []\n",
    "\n",
    "for i in range(len(x_train_dataset)):                   #Make a dataset as a list with length=2 containing the images and the correct classifications\n",
    "    train_dataset.append( (x_train_dataset[i], y_train_dataset[i]) )\n",
    "\n",
    "x_test_dataset = create_set((0, 10000), x_test, 32)     #Repeat for the test set\n",
    "y_test_dataset = create_set((0, 10000), y_test, 32)\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(len(x_test_dataset)):\n",
    "    test_dataset.append( (x_test_dataset[i], y_test_dataset[i]) )"
   ]
  },
  {
   "source": [
    "### We now need to train our network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of Epoch 1\n",
      "Step 200: Train loss: 0.6649, Train Accuracy: 0.7692\n",
      "Step 400: Train loss: 0.4634, Train Accuracy: 0.8355\n",
      "Step 600: Train loss: 0.4353, Train Accuracy: 0.8427\n",
      "Step 800: Train loss: 0.3816, Train Accuracy: 0.8647\n",
      "Step 1000: Train loss: 0.3542, Train Accuracy: 0.8723\n",
      "Step 1200: Train loss: 0.3295, Train Accuracy: 0.8797\n",
      "Step 1400: Train loss: 0.3426, Train Accuracy: 0.8698\n",
      "Step 1600: Train loss: 0.3314, Train Accuracy: 0.8792\n",
      "Step 1800: Train loss: 0.3075, Train Accuracy: 0.8838\n",
      "Start of Epoch 2\n",
      "Step 200: Train loss: 0.2987, Train Accuracy: 0.8895\n",
      "Step 400: Train loss: 0.2940, Train Accuracy: 0.8870\n",
      "Step 600: Train loss: 0.2975, Train Accuracy: 0.8944\n",
      "Step 800: Train loss: 0.2777, Train Accuracy: 0.9002\n",
      "Step 1000: Train loss: 0.2629, Train Accuracy: 0.9041\n",
      "Step 1200: Train loss: 0.2519, Train Accuracy: 0.9052\n",
      "Step 1400: Train loss: 0.2703, Train Accuracy: 0.8989\n",
      "Step 1600: Train loss: 0.2673, Train Accuracy: 0.9033\n",
      "Step 1800: Train loss: 0.2493, Train Accuracy: 0.9034\n",
      "Start of Epoch 3\n",
      "Step 200: Train loss: 0.2491, Train Accuracy: 0.9098\n",
      "Step 400: Train loss: 0.2447, Train Accuracy: 0.9095\n",
      "Step 600: Train loss: 0.2510, Train Accuracy: 0.9081\n",
      "Step 800: Train loss: 0.2332, Train Accuracy: 0.9172\n",
      "Step 1000: Train loss: 0.2181, Train Accuracy: 0.9220\n",
      "Step 1200: Train loss: 0.2109, Train Accuracy: 0.9211\n",
      "Step 1400: Train loss: 0.2289, Train Accuracy: 0.9136\n",
      "Step 1600: Train loss: 0.2268, Train Accuracy: 0.9173\n",
      "Step 1800: Train loss: 0.2129, Train Accuracy: 0.9181\n",
      "Start of Epoch 4\n",
      "Step 200: Train loss: 0.2090, Train Accuracy: 0.9219\n",
      "Step 400: Train loss: 0.2075, Train Accuracy: 0.9234\n",
      "Step 600: Train loss: 0.2105, Train Accuracy: 0.9231\n",
      "Step 800: Train loss: 0.1986, Train Accuracy: 0.9283\n",
      "Step 1000: Train loss: 0.1886, Train Accuracy: 0.9314\n",
      "Step 1200: Train loss: 0.1773, Train Accuracy: 0.9330\n",
      "Step 1400: Train loss: 0.1979, Train Accuracy: 0.9255\n",
      "Step 1600: Train loss: 0.1977, Train Accuracy: 0.9294\n",
      "Step 1800: Train loss: 0.1905, Train Accuracy: 0.9266\n",
      "Start of Epoch 5\n",
      "Step 200: Train loss: 0.1762, Train Accuracy: 0.9377\n",
      "Step 400: Train loss: 0.1794, Train Accuracy: 0.9328\n",
      "Step 600: Train loss: 0.1850, Train Accuracy: 0.9327\n",
      "Step 800: Train loss: 0.1725, Train Accuracy: 0.9367\n",
      "Step 1000: Train loss: 0.1607, Train Accuracy: 0.9417\n",
      "Step 1200: Train loss: 0.1571, Train Accuracy: 0.9414\n",
      "Step 1400: Train loss: 0.1670, Train Accuracy: 0.9356\n",
      "Step 1600: Train loss: 0.1703, Train Accuracy: 0.9370\n",
      "Step 1800: Train loss: 0.1616, Train Accuracy: 0.9383\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(kernels, filters)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_loss_metric = keras.metrics.Mean()\n",
    "accuracy_metric = keras.metrics.Mean()\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Start of Epoch {}\".format(epoch + 1))\n",
    "    train_loss_metric.reset_states()\n",
    "    accuracy_metric.reset_states()\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(model.trainable_weights)\n",
    "            predictions = model(x_batch_train, training=True)\n",
    "            batch_loss_value = loss_fn(y_batch_train, predictions)\n",
    "        grads = tape.gradient(batch_loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_loss_metric.update_state(batch_loss_value)\n",
    "        num_positive_class ,= np.where(np.argmax(predictions, axis=1) == np.argmax(y_batch_train, axis=1))\n",
    "        accuracy_metric.update_state(num_positive_class.size/batch_size)\n",
    "\n",
    "        if int(step+1) % 200 == 0:\n",
    "            train_loss_list.append(train_loss_metric.result())\n",
    "            train_acc_list.append(accuracy_metric.result())\n",
    "            print(\n",
    "                \"Step %d: Train loss: %.4f, Train Accuracy: %.4f\"\n",
    "                % (\n",
    "                    step+1,\n",
    "                    float(train_loss_metric.result()), \n",
    "                    float(accuracy_metric.result())\n",
    "                )\n",
    "                    )\n",
    "            train_loss_metric.reset_states()\n",
    "            accuracy_metric.reset_states()"
   ]
  },
  {
   "source": [
    "### Now that our network is trained, we need to evaluate it on a test dataset. By comparing the accuracy results for training and testing, we can have insight on the generalization potential of our network on unseen data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Test loss: 0.2827\nAverage Test accuracy: 0.9057\n"
     ]
    }
   ],
   "source": [
    "test_loss_metric = keras.metrics.Mean()\n",
    "test_accuracy_metric = keras.metrics.Mean()\n",
    "\n",
    "for step, (x_batch_test, y_batch_test) in enumerate(test_dataset):\n",
    "    predictions = model(x_batch_test, training=True)\n",
    "    batch_loss_value = loss_fn(y_batch_test, predictions)\n",
    "\n",
    "    test_loss_metric.update_state(batch_loss_value)\n",
    "    num_positive_class ,= np.where(np.argmax(predictions, axis=1) == np.argmax(y_batch_test, axis=1))\n",
    "    test_accuracy_metric.update_state(num_positive_class.size/batch_size)\n",
    "\n",
    "print(\"Average Test loss: %.4f\" % (test_loss_metric.result()))\n",
    "print(\"Average Test accuracy: %.4f\" % (test_accuracy_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}